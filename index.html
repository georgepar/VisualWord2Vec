<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <title>Visualword2vec by satwikkottur</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheets/normalize.css" media="screen">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">Visual Word2vec</h1>
      <h2 class="project-tagline">Project page for Visual Word2Vec (vis-w2v)</h2>
      <a href="https://github.com/satwikkottur/VisualWord2Vec" class="btn">View on GitHub</a>
      <a href="https://github.com/satwikkottur/VisualWord2Vec/zipball/master" class="btn">Download .zip</a>
      <a href="https://github.com/satwikkottur/VisualWord2Vec/tarball/master" class="btn">Download .tar.gz</a>
    </section>

    <section class="main-content">
    <h2>Visual Word2Vec (vis-w2v): Learning Visually Grounded Word Embeddings Using Abstract Scenes</h2>
<b>Teaser figure here</b>

<h3>People:</h3>
<p>Satwik Kottur [<a href='https://satwikkottur.github.io/'>Page</a>]
    <br/>Ramakrishna Vedantam [<a href='https://ramakrishnavedantam928.github.io/'>Page</a>]
    <br/>Jos&eacute; M. F. Moura [<a href='https://users.ece.cmu.edu/~moura/'>Page</a>]
    <br/>Devi Parikh [<a href='https://filebox.ece.vt.edu/~parikh/'>Page</a>]</p>

<h3>Abstract:</h3>
<b>This needs change (camera ready)!</b>
<p>We propose a model to learn visually grounded word embeddings (vis-w2v) to capture visual notions of semantic relatedness.
While word embeddings trained using text have been extremely successful, they cannot uncover notions of semantic relatedness implicit in our visual world.
For instance, visual grounding can help us realize that concepts like eating and staring at are related, since when people are eating something, they also tend to stare at the food.
Grounding a rich variety of relations like eating and stare at in vision is a challenging task, despite recent progress in vision.
We realize the visual grounding for words depends on the semantics of our visual world, and not the literal pixels.
We thus use abstract scenes created from clipart to provide the visual grounding.
We find that the embeddings we learn capture fine-grained visually grounded notions of semantic relatedness.
We show improvements over text only word embeddings (word2vec) on three tasks: common-sense assertion classification, visual paraphrasing and text-based image retrieval.</p>

<h3>Paper:</h3>
<p>
    Satwik Kottur*, Ramakrishna Vedantam*, Jos&eacute; M. F. Moura, Devi Parikh <br/>
    <b>Visual Word2Vec (vis-w2v): Learning Visually Grounded Word Embeddings Using Abstract Scenes</b><br/>
    <i>in proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016</i>
    <br/>*=equal contribution
</p>
<p>[<a href='http://arxiv.org/abs/1511.07067'>Paper</a>]
    [<a href='http://satwikkottur.github.io/VisualWord2Vec/'>Project Page</a>]
    [<a href=''>Poster</a>]</p>

<h3>BibTeX:</h3>
<pre><code>@article{DBLP:journals/corr/KotturVMP15,
    author    = {Satwik Kottur and
                Ramakrishna Vedantam and
                Jos{\'{e}} M. F. Moura and
                Devi Parikh},
    title     = {Visual Word2Vec (vis-w2v): Learning Visually Grounded Word Embeddings
                Using Abstract Scenes},
    journal   = {CoRR},
    volume    = {abs/1511.07067},
    year      = {2015},
    url       = {http://arxiv.org/abs/1511.07067},
}</code></pre>


<h3>
<a id="support-or-contact" class="anchor" href="#support-or-contact" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Contact:</h3>
<p>For any questions, feel free to contact Satwik Kottur or Ramakrishna Vedantam.</p>

      <footer class="site-footer">
        <span class="site-footer-owner"><a href="https://github.com/satwikkottur/VisualWord2Vec">Visualword2vec</a> is maintained by <a href="https://github.com/satwikkottur">satwikkottur</a>.</span>

        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the <a href="https://github.com/jasonlong/cayman-theme">Cayman theme</a> by <a href="https://twitter.com/jasonlong">Jason Long</a>.</span>
      </footer>

    </section>

  
  </body>
</html>
