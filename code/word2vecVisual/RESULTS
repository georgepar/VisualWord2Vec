Results using correctedFeatures/:
These have real valued location features are not binary.
The race, flip feature extraction bug is also taken care of.
The original word2vec baseline is from xiaos' ICCV
================================================
Sept 28
=======
PCA (374), Individual words, 10 : 
    (73.89, 72.22) => (74.98, 73.66)
PCA (374), Individual words, 15 : 
    (73.89, 72.22) => (75.27, 73.78)
PCA (374), Individual words, 20 : 
    (73.89, 72.22) => (75.05, 73.55)
PCA (374), Individual words, 25 : 
    (73.89, 72.22) => (75.49, 74.12)
PCA (374), Individual words, 30 : 
    (73.89, 72.22) => (75.47, 74.11)
-------------------------------
PCA (374), Phrases, 10 : 
    (73.89, 72.22) => (74.91, 73.51)
PCA (374), Phrases, 15 : 
    (73.89, 72.22) => (74.33, 72.72)
PCA (374), Phrases, 20 : 
    (73.89, 72.22) => (74.25, 72.71)
PCA (374), Phrases, 25 : 
    (73.89, 72.22) => (74.25, 72.74)
PCA (374), Phrases, 30 : 
    (73.89, 72.22) => (74.39, 72.93)
-------------------------------
1222, Individual words, 10 :
    (73.89, 72.22) => (74.78, 73.45)
1222, Individual words, 15 :
    (73.89, 72.22) => (75.10, 73.71)
1222, Individual words, 20 :
    (73.89, 72.22) => (75.69, 74.34)
1222, Individual words, 25 :
    (73.89, 72.22) => (75.88, 74.47)
1222, Individual words, 30 :
    (73.89, 72.22) => (75.61, 74.24)
-------------------------------
1222, Phrases, 10 : 
    (73.89, 72.22) => (74.10, 72.49)
1222, Phrases, 15 : 
    (73.89, 72.22) => (74.17, 72.61)
1222, Phrases, 20 : 
    (73.89, 72.22) => (74.78, 73.29)
1222, Phrases, 25 : 
    (73.89, 72.22) => (74.55, 73.09)
1222, Phrases, 30 : 
    (73.89, 72.22) => (74.46, 73.01)
================================================
================================================
(Multi models for P,S,R)

PCA (374), Individual words, 10 : 
    (73.89, 72.22) => (74.96, 73.77)
PCA (374), Individual words, 15 : 
    (73.89, 72.22) => (75.23, 74.00)
PCA (374), Individual words, 20 : 
    (73.89, 72.22) => (75.11, 73.89)
PCA (374), Individual words, 25 : 
    (73.89, 72.22) => (75.61, 74.37)
PCA (374), Individual words, 30 : 
    (73.89, 72.22) => (75.61, 74.57)
-------------------------------
PCA (374), Phrases, 10 : 
    (73.89, 72.22) => (74.69, 73.35)
PCA (374), Phrases, 15 : 
    (73.89, 72.22) => (74.56, 73.07)
PCA (374), Phrases, 20 : 
    (73.89, 72.22) => (74.44, 73.10)
PCA (374), Phrases, 25 : 
    (73.89, 72.22) => (74.07, 72.56)
PCA (374), Phrases, 30 : 
    (73.89, 72.22) => (74.39, 73.08)
-------------------------------
1222, Individual words, 10 :
    (73.89, 72.22) => (74.74, 73.52)
1222, Individual words, 15 :
    (73.89, 72.22) => (75.02, 73.91)
1222, Individual words, 20 :
    (73.89, 72.22) => (75.23, 74.10)
1222, Individual words, 25 :
    (73.89, 72.22) => (75.92, 74.75)
1222, Individual words, 30 :
    (73.89, 72.22) => (75.67, 74.57)
1222, Individual words, 35 :
    (73.89, 72.22) => (75.66, 74.57)
-------------------------------
1222, Phrases, 10 : 
    (73.89, 72.22) => (74.05, 72.47)
1222, Phrases, 15 : 
    (73.89, 72.22) => (73.96, 72.34)
1222, Phrases, 20 : 
    (73.89, 72.22) => (75.00, 73.70)
1222, Phrases, 25 : 
    (73.89, 72.22) => (74.57, 73.21)
1222, Phrases, 30 : 
    (73.89, 72.22) => (74.46, 73.20)
================================================
Varying the dimension of the hidden layer (5 runs each)

Model : Individual words, Multi, 1222, 10

20: (68.60, 67.58) => (71.28, 70.60)
30: (71.87, 70.69) => (74.33, 73.37)
40: (73.41, 72.12) => (74.86, 73.99)
50: (73.98, 72.59) => (75.41, 74.31)
100:(75.03, 73.28) => (75.67, 74.31)
200:(73.89, 72.22) => (74.74, 73.52)
300:(74.05, 72.61) => (74.65, 73.31)
400:(73.51, 72.00) => (74.21, 72.90)

Model : Individual words, Multi, 1222, 25

20: (68.38, 67.36) => (74.47, 73.60)
30: (71.99, 70.80) => (76.01, 74.76)
40: (73.12, 71.87) => (76.30, 75.17)
50: (74.05, 72.59) => (76.28, 75.05)
100:(74.96, 73.28) => (76.76, 75.52)
200:(73.89, 72.22) => (75.92, 74.75)
300:(74.01, 72.59) => (75.67, 74.48)
400:(73.57, 72.05) => (75.39, 74.07)

================================================
Varying the number of training examples per R
Model : Individual words, Multi, 1222, 10
1 : 
    (70.35, 68.43) => (70.39, 68.52)
2 : 
    (70.65, 68.68) => (70.52, 68.50)
5 :
    (72.61, 71.10) => (73.56, 72.33)
10 : 
    (73.30, 71.78) => (74.53, 73.21)
12 :
    (73.71, 72.11) => (74.59, 73.45)
14 :
    (73.63, 72.11) => (74.55, 73.07)
16 :
    (73.88, 72.21) => (, 73.7)
18:
    (73.88, 72.21) => (74.99, 73.74)
20 : 
    (73.89, 72.22) => (74.74, 73.52)

Model : Individual words, Multi, 1222, 25
1 : 
    (70.35, 68.43) => (70.36, 68.46)
2 : 
    (70.65, 68.68) => (70.48, 68.44)
5 :
    (72.61, 71.10) => (72.53, 71.08)
10 : 
    (73.30, 71.78) => (74.89, 73.52)
12 :
    (73.71, 72.11) => (74.93, 73.88)
14 :
    (73.63, 72.11) => (75.09, 74.13)
16 :
    (73.88, 72.21) => (, 73.74)
18 :
    (73.88, 72.21) => (75.11, 73.68)
20 : 
    (73.89, 72.22) => (75.92, 74.75)

================================================

ICCV (Textual + visual) : 
    Single model : 
        Baseline Text only : 72.22
        Text only: 74.49

        Baseline Vision + Text: 73.60
        Vision + Text : 74.21

    Multi model:
        Baseline Text only : 72.22
        Text only: 74.80

        Baseline Vision + Text: 73.60
        Vision + Text : 74.21

================================================
Wikipedia text training
1222, Individual Words, Multi
10:
25: (64.41, 63.18) => (74.47, 73.47)
35: 
================================================
VP task:
Baseline text accuracies : 94.55, 94.60, 94.38, 94.38, 94.54
VisualWord2Vec: 
================================================


================================================
Wikipedia text training
1222, Individual Words, Multi
10:
25: (64.41, 63.18) => (74.47, 73.47)
35: 
================================================
VP task:
Baseline text accuracies : 94.55, 94.60, 94.38, 94.38, 94.54
VisualWord2Vec: 
================================================

